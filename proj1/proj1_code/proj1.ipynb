{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 6320 Project 1: Images as Functions and Pytorch Introduction\n",
    "\n",
    "All projects in this course will be done with these iPython notebooks. These are convenient ways for you to quickly and easily interact with the code. A notebook contains many blocks of code, each of which can be run independently. You can run a cell with ctrl+enter or shift+enter (to move to the next cell).\n",
    "\n",
    "\n",
    "You are expected to complete this notebook with lines of code, plots and texts. You will need to create new cells with original code or text for your analyses and explanations. Explain what you do and analyze your results. This assignment has a total of 100 points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief\n",
    "    Due: 9/26/2023 11:59PM\n",
    "    Project materials including writeup template proj1_6320.zip\n",
    "    Hand-in: through Canvas\n",
    "    Required files: <your_uid>.zip.(Please begin with 'u' for your uid)\n",
    "<!---<your_uid>_proj1.pdf--> \n",
    "\n",
    "## Overview\n",
    "The goal of this assignment is to apply basic operation on the images. An image can be viewed as a function of its individual pixels. Given a pixel coordination, we can get its value. In Part 1, you will practice how to get the values of the specified pixels in the images. In Part 2, you are expected to apply 2D transformation on the pixel coordination. In Part 3, you can use the transformation you wrote in Part 2 to preprocess the image in the dataset for a neural model. \n",
    "\n",
    "This project is intended to familiarize you with Python, PyTorch, and image transformation. Once you have created an image transformation function, it is relatively straightforward to create a new image with the transformation. If you don’t already know Python, you may find this [resource](https://docs.python.org/3/tutorial/) helpful. If you’re unfamiliar with PyTorch, the tutorials from the [official website](https://pytorch.org/tutorials/) are useful.\n",
    "\n",
    "## Setup\n",
    "   0. Unzip proj1_6320.zip and go to proj1_6320 directory.\n",
    "      - You can run `unzip proj1_6320.zip && cd proj1_6320` in your terminal.\n",
    "   1. Install [Miniconda](https://docs.conda.io/en/latest/miniconda.html). It doesn’t matter whether you use Python 2 or 3 because we will create our own environment that uses 3 anyway.\n",
    "   2. Create a conda environment using the appropriate command. On Windows, open the installed “Conda prompt” to run the command. On MacOS and Linux, you can just use a terminal window to run the command, Modify the command based on your OS (linux, mac, or win): `conda env create -f proj1_env_<OS>.yml`.\n",
    "    - NOTE that proj1_env_<OS>.yml is inside the project folder.\n",
    "   3. This should create an environment named ‘proj1’. Activate it using the Windows command, activate proj1 or the MacOS / Linux command, source activate proj1\n",
    "   4. Install the project package, by running `pip install -e .` inside the repo folder.\n",
    "   5. Run the notebook using `jupyter notebook` under *proj1_6320* directory.\n",
    "   6. Ensure that all sanity checks are passing by running pytest tests inside the repo folder.\n",
    "   7. Generate the zip folder for the code portion of your submission once you’ve finished the project using \n",
    "    \n",
    "        `python zip_submission.py --uid <your_uid>` \n",
    "        \n",
    "<!---(don’t forget to submit your report, too!).--->\n",
    "    \n",
    "    \n",
    "## Writeup\n",
    "For this project, this notebook itself is the report. You must run all your cells before you hand in it. You code, results, visualization, and discussion will be used for the grading. You will be deducted poins if the results are not shown in this notebook. Do not change the order of the cells. You can add cells in need. You can copy a cell and run it separately if you need to run a cell multiple times and thus every result is displayed in the cell.\n",
    "<!--For this project (and all other projects), you must do a project report using the template slides provided to you. Do not change the order of the slides or remove any slides, as this will affect the grading process on Gradescope and you will be deducted points. In the report you will describe your algorithm and any decisions you made to write your algorithm a particular way. Then you will show and discuss the results of your algorithm. The template slides provide guidance for what you should include in your report. A good writeup doesn’t just show results–it tries to draw some conclusions from the experiments. You must convert the slide deck into a PDF for your submission.\n",
    "\n",
    "If you choose to do anything extra, add slides after the slides given in the template deck to describe your implementation, results, and analysis. Adding slides in between the report template will cause issues with Gradescope, and you will be deducted points. You will not receive full credit for your extra credit implementations if they are not described adequately in your writeup.\n",
    "-->\n",
    "\n",
    "## Rubric\n",
    "    +3 pts: for each question in Part 1(+36 pts in total).\n",
    "    +5 pts: get_scaling_matrix() in Part 2\n",
    "    +5 pts: get_rotation_matrix() in Part 2\n",
    "    +5 pts: get_translation_matrix() in Part 2\n",
    "    +5 pts: multiplication order of sequential transformation in Part 2\n",
    "    +24 pts: get_transform() in Part 2.\n",
    "    +15 pts: ImageDataset() in Part 3.\n",
    "    +5 pts: comparision of the predictions due to different preprocessing parameters in part 3.\n",
    "\n",
    "    -5*n pts: Lose 5 points for every time you do not follow the instructions for the hand-in format.\n",
    "    \n",
    "Submission Format\n",
    "\n",
    "This is very important as you will lose 5 points for every time you do not follow the instructions. You will attach two items in your submission on Canvas:\n",
    "\n",
    "1. <your_uid>.zip containing:\n",
    "    - proj1_code/ - directory containing all your code for this assignment\n",
    "    - README.txt - (optional) if you implement any new functions other than the ones we define in the skeleton code (e.g. any extra credit implementations), please describe what you did and how we can run the code. We will not award any extra credit if we can’t run your code and verify the results.\n",
    "<!--2. <your_uid>_proj1.pdf - your report-->\n",
    "\n",
    "Do not install any additional packages inside the conda environment. The TAs will use the same environment as defined in the config files we provide you, so anything that’s not in there by default will probably cause your code to break during grading. Do not use absolute paths in your code or your code will break. Use relative paths like the starter code already does. Failure to follow any of these instructions will lead to point deductions. Create the zip file using python zip_submission.py --uid <your_uid> (it will zip up the appropriate directories/files for you!) and hand it in with your PDF through Canvas.\n",
    "\n",
    "### Credits\n",
    "Assignment developed by Xin Yu, Tucker Hermans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Part 1: Images as Functions](https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html)\n",
    "\n",
    "To better understand the inherent properties of images and the technical procedure used to manipulate and process them, we can think of an image, which is comprised of individual pixels, as a function, f. Each pixel also has its own value. For a grayscale image, each pixel would have an intensity between 0 and 255, with 0 being black and 255 being white. f(x,y) would then give the intensity of the image at pixel position (x,y), assuming it is defined over a rectangle, with a finite range: f: [a,b] x [c,d] → [0, 255].\n",
    "\n",
    "<img src=\"../extra_files/image_as_function_gray.jpg\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "A color image is just a simple extension of this. f(x,y) is now a vector of three values instead of one. Using an RGB image as an example, the colors are constructed from a combination of Red, Green, and Blue (RGB). Therefore, each pixel of the image has three channels and is represented as a 1x3 vector. Since the three colors have integer values from 0 to 255, there are a total of 256*256*256 = 16,777,216 combinations or color choices.\n",
    "\n",
    "f(x,y) can be represented as three functions merged together.\n",
    "<img src='../extra_files/image_as_function_rgb_function.jpg' width=\"200\"/>\n",
    "<img src='../extra_files/image_as_function_rgb.jpg' width=\"300\"/>\n",
    "\n",
    "### Input images\n",
    "\n",
    "Find two interesting images to use. They should be color. You might find some classic vision\n",
    " examples in http://sipi.usc.edu/database/database.php?volume=misc Make sure they are not larger than 512x512.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "\n",
    "from utils import load_image, save_image\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "image1 = load_image('../data/1a_dog.bmp')\n",
    "image2 = load_image('../data/1b_cat.bmp')\n",
    "\n",
    "# display the dog and cat images\n",
    "plt.figure(figsize=(3,3)); plt.imshow((image1*255).astype(np.uint8));\n",
    "plt.figure(figsize=(3,3)); plt.imshow((image2*255).astype(np.uint8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Color planes\n",
    "\n",
    "You are expected to get image planes for the specified channels, visualize them and answer the questions.\n",
    "1. Swap the red and blue pixels of image 1 (Store as M1rb).\n",
    "   - Output: new image\n",
    "2. Create a monochrome image (M1g) by selecting the green channel of image 1\n",
    "   - Output: new image\n",
    "3. Create a monochrome image (M1r) by selecting the red channel of image 1\n",
    "   - Output: new image \n",
    "4. Which looks more like what you’d expect a monochrome image to look like? Would you expect a computer vision algorithm to work on one better than the other? \n",
    "   - Output: response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## impletement the code and show your results here\n",
    "# M1rb\n",
    "# M1g\n",
    "# M1r \n",
    "############################\n",
    "### TODO: YOUR CODE HERE ###\n",
    "raise NotImplementedError('Operation on color planes needs to be implemented')\n",
    "### END OF STUDENT CODE ####\n",
    "############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); plt.imshow((M1rb*255).astype(np.uint8));\n",
    "plt.figure(); plt.imshow(M1g,cmap='gray'); # use gray for monochrome images.\n",
    "plt.figure(); plt.imshow(M1r,cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image('../results/part1/M1rb.jpg', M1rb)\n",
    "save_image('../results/part1/M1g.jpg', M1g)\n",
    "save_image('../results/part1/M1r.jpg', M1r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Replacement of pixels\n",
    "\n",
    "Take the inner square of 100x100 pixels of monochrome version of image 1 and insert them into the monochrome version of image 2.\n",
    "- new image (saved as replacement.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "### TODO: YOUR CODE HERE ###\n",
    "raise NotImplementedError('Replacement of pixles needs to be implemented. Please show and save resutls')\n",
    "### END OF STUDENT CODE ####\n",
    "############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Arithmetic and Geometric operation\n",
    "\n",
    "1. What is the min and max of the pixel values of M1g? What is the mean? What is the standard deviation? And how did you compute these?\n",
    "   - Output: response\n",
    "2. Subtract the mean from all the pixels, then divide by the standard deviation, then multiply by 10 (if your image is zero to 255) or by 0.05 (if your image ranges from 0.0 to 1.0). Now add the mean back in.\n",
    "   - Ouptut: new image (saved as subtract.jpg)\n",
    "3. Shift M1g to the left by 2 pixels. \n",
    "   - Output: new image (saved as shift1.jpg)\n",
    "4. Subtract the shifted version of M1g from the original and make sure that the values are legal (what do negative numbers for pixels mean anyway?).\n",
    "   - Output: new image (saved as shift2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "### TODO: YOUR CODE HERE ###\n",
    "raise NotImplementedError('Arithmetic and Geometric operation needs to be implemented. Please show and save resutls')\n",
    "### END OF STUDENT CODE ####\n",
    "############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Noise\n",
    "\n",
    "1. Take the original colored image and strat adding Gaussian noise to the pixels in the green channel. Increase sigma until the noise is somewhat visible.\n",
    "   - Ouput: new image (saved as noise1.jpg). Response: what is sigma? \n",
    "2. Now, instead add that amount of noise to the blue channel.\n",
    "   - Output: new image (saved as noise2.jpg)\n",
    "3. Which looks better? Why?\n",
    "   - Output: response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "### TODO: YOUR CODE HERE ###\n",
    "raise NotImplementedError('Noise on images needs to be implemented. Please show and save resutls')\n",
    "### END OF STUDENT CODE ####\n",
    "############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2: 2D transformations\n",
    "\n",
    "### 2.1 Homogenous Coordinates\n",
    "For 2D transformation, we are able to multiply a 2x2 transformation matrix. Take the scale transformation as example, it can be written as a matrix mutiplication on the coordinate of a point.\n",
    "<img src='../extra_files/scale_matrix2d_2.jpg' width='250'>\n",
    "\n",
    "To perform a sequence of transformation such as translation followed by rotation and scaling, we need to follow a sequential process. For example,\n",
    "\n",
    "   - Translate the coordinates,\n",
    "   - Rotate the translated coordinates, and then\n",
    "   - Scale the rotated coordinates to complete the composite transformation.\n",
    "\n",
    "To shorten this process, we will use 3×3 transformation matrix instead of 2×2 transformation matrix. To convert a 2×2 matrix to 3×3 matrix, we have to add an extra dummy coordinate W.\n",
    "\n",
    "In this way, we can represent the point by 3 numbers instead of 2 numbers, which is called Homogenous Coordinate system. In this system, we can represent all the transformation equations in matrix multiplication. Any Cartesian point P (X,Y) can be converted to homogenous coordinates by P’ (Xh, Yh, h).\n",
    "\n",
    "<img src='../extra_files/homogeneous_transformation.jpg' width='600'>\n",
    "\n",
    "Thus we can follow a sequetial process to peform a sequence of transformations. For example,\n",
    "\n",
    "<img src='../extra_files/homogeneous_transformation_composition.jpg' width='500'>\n",
    "\n",
    "Please see the [slides](https://dellaert.github.io/19F-4476/Slides/S02-Geometry.pdf) for the mathmatical details.\n",
    "\n",
    "#### 2.1.1 2D transformation \n",
    "1. We will apply the 2D transformation to image preprocessing, which are composition of multiple operations. Before that, you are expected to implement scaling, rotation, translation operation seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaling_matrix(scale_x, scale_y):\n",
    "    \"\"\"Generaete scaling transformation matrix\"\"\"\n",
    "    '''\n",
    "        Input:\n",
    "            scale_x: scaling factor along x axis\n",
    "            scale_y: scaling factor along y axis\n",
    "        Output: \n",
    "            scaling_matrix: 3x3 matrix\n",
    "    '''\n",
    "    ############################\n",
    "    ### TODO: YOUR CODE HERE ###\n",
    "    raise NotImplementedError(' get_scaling_matrix() funciton needs to be implemented.')\n",
    "    ### END OF STUDENT CODE ####\n",
    "    ############################\n",
    "\n",
    "def get_rotation_matrix(rot_angle_degree):\n",
    "    \"\"\"Generaete rotation transformation matrix\"\"\"\n",
    "    '''\n",
    "        Input:\n",
    "            rot_angle_degree: rotation angle in degree\n",
    "        Output: \n",
    "            rotation_matrix: 3x3 matrix\n",
    "    '''\n",
    "    ############################\n",
    "    ### TODO: YOUR CODE HERE ###\n",
    "    raise NotImplementedError(' get_rotation_matrix() funciton needs to be implemented.')\n",
    "    ### END OF STUDENT CODE ####\n",
    "    ############################\n",
    "    \n",
    "def get_translation_matrix(delta_x, delta_y):\n",
    "    \"\"\"Generaete scaling transformation matrix\"\"\"\n",
    "    '''\n",
    "        Input:\n",
    "            delta_x: movement along x axis\n",
    "            delta_y: movement along y axis\n",
    "        Output: \n",
    "            translation_matrix: 3x3 matrix\n",
    "    '''\n",
    "    ############################\n",
    "    ### TODO: YOUR CODE HERE ###\n",
    "    raise NotImplementedError(' get_translation_matrix() funciton needs to be implemented.')\n",
    "    ### END OF STUDENT CODE ####\n",
    "    ############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Given a point, test the transformations seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transformation():\n",
    "    point = np.array([2,2])\n",
    "    point_homo = np.hstack((point, np.ones(1)))\n",
    "    point1 = np.dot(get_scaling_matrix(0.5, 1), point_homo)\n",
    "    np.testing.assert_array_equal(point1, [1,2,1], \"Please check get_scaling_matrix() function\")\n",
    "    \n",
    "    point2 = np.dot(get_rotation_matrix(45), point_homo)\n",
    "    np.testing.assert_allclose(point2, [0,np.sqrt(8),1], atol=1e-5, rtol=0, \n",
    "                               err_msg=\"Please check get_rotation_matrix() function\", )\n",
    "    \n",
    "    point3 = np.dot(get_translation_matrix(-1,1), point_homo)\n",
    "    np.testing.assert_array_equal(point3, [1,3,1], \"Please check get_translation_matrix() function\")\n",
    "\n",
    "test_transformation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Composition of tranformation: does the multiplication order matter? \n",
    "    - Please try different order and verify your answer with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "### TODO: YOUR CODE HERE ###\n",
    "raise NotImplementedError('Trial on multiplication order needs to be implemented.')\n",
    "### END OF STUDENT CODE ####\n",
    "############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 the 2D transformation on images\n",
    "In a computer vision/image processing task, we always needs to apply 2D transformation on the images, such as crop, rotation, flip and so on. \n",
    "\n",
    "    - crop: crop a rectangular portion of any image\n",
    "    - rotation: rotate an image around a given center\n",
    "    \n",
    "We show a bounding box in red in an example image, which centers at the red dot.\n",
    "<img src='../extra_files/bb_dog1.jpg' width=300>\n",
    "The crop operation will crop the image and get the image area inside the bounding box:\n",
    "<img src='../extra_files/crop_dog11.jpg' width=150>\n",
    "If the rotation operation is required, we will rotate the original image firstly and then crop it. \n",
    "<img src='../extra_files/rot_dog11.jpg' width=150>\n",
    "\n",
    "1. You are expected to generate the transformation matrix for image crop with rotation enabled. The cropped portion of an image is defined with a bounding box(bb) represented as:\n",
    "     - center of the bounding box: (x,y) in the original image\n",
    "     - scale: actual_bb_edge_length/200\n",
    "              we define: for a scale of 1, we get a square bounding box of size `[200 x 200]'. \n",
    "              to get a target bounding box size, we set the scale = target_bb_size / 200.\n",
    "              hence the shape is `[200 x scale, 200 x scale]`\n",
    "              \n",
    "   HINT: \n",
    "        - To keep the bb center, you need to rotate the image around.\n",
    "        - Please pay attention to the order of muplication.\n",
    "        - As seen in the above figure, the pixel (0,0) locate at the up-left corner of the image, which is different from the the 2D coordinate system. To match the direction of rotation of an image, you need to assign\n",
    "          rot = - rot.\n",
    "        - You can use the functions defined in the previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(center, scale, res, rot=0):\n",
    "    \"\"\"Generate transformation matrix that tranform a point in an image for image cropping\"\"\"\n",
    "    ''' \n",
    "        bounding box representation:\n",
    "             center: position of the bounding box center in the original image\n",
    "             scale: scale = actual_bb_edge_length/200\n",
    "                    thence, the shape is [200 x scale, 200 x scale]\n",
    "        res: the resolution of the cropped image. It can be different from the bounding box size, and you will\n",
    "             need to apply the scaling operation to make the cropped image get the 'res' resolution.\n",
    "        rot: rotation angle in degress. \n",
    "        output: 3 x 3 matrix\n",
    "        \n",
    "        NOTE:\n",
    "        - The coordinates from the original image must be rotated and mapped to the cropped imaged while also\n",
    "        accounting for scaling.\n",
    "        - For Example the top left corner (0,0) of the cropped image may have non-zero coordinates in the original image.\n",
    "        This needs to be accounted for translation, rotation and scaling to match the target resolution.\n",
    "        - Please pay attention to the order of muplication.\n",
    "        - The scale of the bb is different from res. The scale defines the size of the bounding box and inturn the portion\n",
    "        of the image to be cropped. The res defines the target size of the cropped image i.e. the cropped image has to be\n",
    "        scaled to meet the desired res.\n",
    "        - You can use the functions defined in the previous sections.\n",
    "    '''\n",
    "    ############################\n",
    "    ### TODO: YOUR CODE HERE ###\n",
    "    raise NotImplementedError(' get_transform() funciton needs to be implemented.')\n",
    "    ### END OF STUDENT CODE ####\n",
    "    ############################\n",
    "\n",
    "def transform(pt, center, scale, res, rot=0, invert=0):\n",
    "    \"\"\"Transform pixel location to different reference.\"\"\"\n",
    "    '''\n",
    "    NOTE:\n",
    "         bounding box representation:\n",
    "             center: position of the bounding box center in the original image\n",
    "             scale: scale = actual_bb_edge_length/200\n",
    "                    thence, the shape is [200xscale, 200xscale]\n",
    "     Tranform:\n",
    "         translate:\n",
    "             from orig_img coord to bbox coord, if invert=0\n",
    "             from bbox coord to orig_img, if invert=1\n",
    "    '''\n",
    "    t = get_transform(center, scale, res, rot=rot)\n",
    "    if invert:\n",
    "        t = np.linalg.inv(t)\n",
    "    new_pt = np.array([pt[0]-1, pt[1]-1, 1.]).T\n",
    "    new_pt = np.dot(t, new_pt)\n",
    "    return new_pt[:2].astype(int)+1\n",
    "\n",
    "def flip_img(img):\n",
    "    \"\"\"Flip rgb images or masks.\n",
    "    channels come last, e.g. (256,256,3).\n",
    "    \"\"\"\n",
    "    img = np.fliplr(img)\n",
    "    return img\n",
    "\n",
    "def crop(img, center, scale, res, rot=0):\n",
    "    \"\"\"Crop image according to the supplied bounding box.\"\"\"\n",
    "    '''\n",
    "     NOTE:\n",
    "         bounding box representation:\n",
    "             center: position of the bounding box center in the original image\n",
    "             scale: scale = actual_bb_edge_length/200\n",
    "                    thence, the shape is [200xscale, 200xscale]\n",
    "     Tranform:\n",
    "         translate:\n",
    "             from orig_img coord to bbox coord, if invert=0\n",
    "             from bbox coord to orig_img, if invert=1\n",
    "    '''\n",
    "    # Upper left point\n",
    "    ul = np.array(transform([1, 1], center, scale, res, invert=1))-1\n",
    "    # Bottom right point\n",
    "    br = np.array(transform([res[0]+1,\n",
    "                             res[1]+1], center, scale, res, invert=1))-1\n",
    "    # Padding so that when rotated proper amount of context is included\n",
    "    pad = int(np.linalg.norm(br - ul) / 2 - float(br[1] - ul[1]) / 2)\n",
    "    if not rot == 0:\n",
    "        ul -= pad\n",
    "        br += pad\n",
    "\n",
    "    new_shape = [br[1] - ul[1], br[0] - ul[0]]\n",
    "    if len(img.shape) > 2:\n",
    "        new_shape += [img.shape[2]]\n",
    "    new_img = np.zeros(new_shape)\n",
    "\n",
    "    # Range to fill new array\n",
    "    new_x = max(0, -ul[0]), min(br[0], len(img[0])) - ul[0]\n",
    "    new_y = max(0, -ul[1]), min(br[1], len(img)) - ul[1]\n",
    "    # Range to sample from original image\n",
    "    old_x = max(0, ul[0]), min(len(img[0]), br[0])\n",
    "    old_y = max(0, ul[1]), min(len(img), br[1])\n",
    "    new_img[new_y[0]:new_y[1], new_x[0]:new_x[1]] = img[old_y[0]:old_y[1],\n",
    "                                                        old_x[0]:old_x[1]]\n",
    "\n",
    "    if not rot == 0:\n",
    "        # Remove padding\n",
    "        new_img = scipy.misc.imrotate(new_img, rot)\n",
    "        #new_img = scipy.skimage.transform.rotate(new_img, rot)\n",
    "        new_img = new_img[pad:-pad, pad:-pad]\n",
    "\n",
    "    new_img = scipy.misc.imresize(new_img, res)\n",
    "    return new_img        \n",
    "\n",
    "def rgb_processing(rgb_img, center, scale, res, rot, flip):\n",
    "    \"\"\"Process rgb image and do augmentation.\"\"\"\n",
    "    rgb_img = crop(rgb_img, center, scale,\n",
    "                  res, rot=rot)\n",
    "    # flip the image\n",
    "    if flip:\n",
    "        rgb_img = flip_img(rgb_img)\n",
    "    #rgb_img = np.transpose(rgb_img.astype('float32'),(2,0,1))/255.0\n",
    "    return rgb_img/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. run the following cell and you should get similar results as shown at the begging of section 2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,w,c=image1.shape\n",
    "im1 = np.zeros([h+100,w+100,3])\n",
    "im1[50:h+50,50:w+50,:] = image1\n",
    "plt.figure(figsize=(3,3)); plt.imshow((im1*255).astype(np.uint8));\n",
    "plt.scatter(130, 155, marker='.', color='r', s =50)\n",
    "rectangle=plt.Rectangle((30,55), 200, 200, color='r', fill=False)\n",
    "plt.gca().add_patch(rectangle)\n",
    "\n",
    "im1_rot = rgb_processing(image1, [80,105], 1, [200,200], 0, 0)\n",
    "plt.figure(figsize=(3,3)); plt.imshow((im1_rot*255).astype(np.uint8));\n",
    "plt.scatter(100, 100, marker='.', color='r', s =50)\n",
    "\n",
    "im1_rot = rgb_processing(image1, [80,105], 1, [200,200], 30, 0)\n",
    "plt.figure(figsize=(3,3)); plt.imshow((im1_rot*255).astype(np.uint8));\n",
    "plt.scatter(100, 100, marker='.', color='r', s =50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: PyTorch \n",
    "\n",
    "### Instantiate model & dataset\n",
    "- Dataloader. \n",
    "    You will now implement creating images again but using PyTorch. The ImageDataset class will load a list of images from data/ directory and apply 2D tranformation on them using rgb_processing() function. Refer to this [tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) for additional information on data loading & processing.\n",
    "\n",
    "\n",
    "- Model. \n",
    "    Next, we will load a pretrained model instead of our own implementation. This model is trained on ImageNet for classification task.  [ImageNet](http://www.image-net.org/about-overview) consists of over 15 million labeled high-resolution images in 1000 classes. The output of the evaluation using this model is a array of probabilities that the input image belongs to a certain class. We will show the top-5 prediction for each input image.\n",
    "\n",
    "    Refer to this [tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) for additional information on defining neural networks using PyTorch and this [page](https://gist.github.com/RamonYeung/988945c805938636fc85c5385bd3d1b4) for more methods to load a pretrained model.\n",
    "\n",
    "\n",
    "3.1. You are expected to implement the ImageDataset() class. You must apply the defined rgb_preprocessing() to image for the transforms. There are three sets of parameters in rgb_preprocessing() to define the tranformations:\n",
    "   - define bounding box for crop: center, scale, res.\n",
    "     <span style=\"color:red\">Specify the output res as [224,224].<span>\n",
    "   - rot: <span style=\"color:red\">You must use a random value in the range [0,90].<span>\n",
    "   - flip: <span style=\"color:red\">You must randomly set it to be 0 or 1.<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from os import listdir\n",
    "from os.path import isfile, join, basename\n",
    "import random\n",
    "\n",
    "class ImageDataset(data.Dataset):\n",
    "    \"\"\"images dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, image_dir: str) -> None:\n",
    "        \"\"\"\n",
    "        ImageDataset class constructor.\n",
    "\n",
    "        You must get the absolute paths of all images under image_dir\n",
    "\n",
    "         Args:\n",
    "         - image_dir: string specifying the directory containing images\n",
    "        \"\"\"\n",
    "        self.images = [join(image_dir,f) for f in listdir(image_dir) if isfile(join(image_dir, f))]\n",
    "        ############################\n",
    "        ### TODO: YOUR CODE HERE ###\n",
    "        # Define additional variables if needed here \n",
    "        ### END OF STUDENT CODE ####\n",
    "        ############################\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns number of images in dataset.\"\"\"\n",
    "\n",
    "        ############################\n",
    "        ### TODO: YOUR CODE HERE ###\n",
    "        raise NotImplementedError('`__len__` function needs to be implemented')\n",
    "        ### END OF STUDENT CODE ####\n",
    "        ############################\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "         Returns the images at index `idx`.\n",
    "\n",
    "         Since self.images contain paths to the images, you should read the images here and \n",
    "         apply preprocess to each image with rgb_processing() function.\n",
    "         Make sure you transpose the dimensions so that image_a and image_b are of\n",
    "         shape (c, h, w) instead of the typical (h, w, c), and convert them to\n",
    "         torch Tensors.\n",
    "         \n",
    "         You must apply the defined rgb_preprocessing() to image for the transforms. You can \n",
    "         specify additional transforms if you want to. There are three sets of parameters in rgb_preprocessing()\n",
    "         to define the tranformations:\n",
    "             - define bounding box for crop: center, scale, res\n",
    "             - rot: You must use a random value in the range [0,90].\n",
    "             - flip: You must randomly set it to be 0 or 1.\n",
    "\n",
    "         Args\n",
    "         - idx: int specifying the index at which data should be retrieved\n",
    "         Returns\n",
    "         - orig_im\n",
    "         - im: Tensor of shape (c, h, w) for image after applying rgb_preprocessing()\n",
    "         - im_name: the base name of the fetched image\n",
    "         \"\"\"\n",
    "\n",
    "        ############################\n",
    "        ### TODO: YOUR CODE HERE ###\n",
    "        raise NotImplementedError('`__getitem__` function  needs to be implemented')\n",
    "        ### END OF STUDENT CODE ####\n",
    "        ############################\n",
    "\n",
    "        return im_orig, im, im_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Run the next cell. It will visualize the origial images and its correspondance after preprocessing. You will get the top-5 prediction of the classication for each image. As the image preprocessing are randomly set, what is the difference of the predictions for multiple runs? Show the results for different run and write down you observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet101\n",
    "\n",
    "data_root = '../data' # if you're using additional data, make sure to change this to '../additional_data'\n",
    "dataset = ImageDataset(data_root)\n",
    "data_size = len(dataset)\n",
    "print(\"There are {} images under {}\".format(data_size, data_root))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset)\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "# First, load the model\n",
    "resnet = resnet101(pretrained=True)\n",
    "with open('../imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "f = plt.figure(figsize=(10,30))\n",
    "# Second, put the network in eval mode\n",
    "resnet.eval()\n",
    "for i in range(data_size):\n",
    "    [im_orig, im, im_name] = next(data_iter)\n",
    "#     print(type(im_orig), im_orig.shape)\n",
    "#     print(type(im), im.shape)\n",
    "    ax = f.add_subplot(data_size, 2, i*2+1)\n",
    "    ax.imshow(im_orig.squeeze())\n",
    "    im_show = im.squeeze().permute([1,2,0])\n",
    "    ax = f.add_subplot(data_size, 2, i*2+2)\n",
    "    ax.imshow(im_show)\n",
    "    \n",
    "    # Third, carry out model inference\n",
    "    out = resnet(im)\n",
    "\n",
    "    # Forth, print the top 5 classes predicted by the model\n",
    "    _, indices = torch.sort(out, descending=True)\n",
    "    scores = torch.nn.functional.softmax(out, dim=1)[0] \n",
    "    print(\"Classify the image: {} and show top 5 prediction results and their scores \".format(im_name[0]))\n",
    "    for idx in indices[0][:5]:\n",
    "        print('\\t %s: %.3f'%(labels[idx], scores[idx].item()))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
